<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Mateo Juliani</title>
    <link>https://mateojuliani.github.io/posts/</link>
    <description>Recent content in Posts on Mateo Juliani</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 21 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mateojuliani.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Short Thoughts on Various Probabilistic ML Papers</title>
      <link>https://mateojuliani.github.io/posts/2025_12_14_6701_readings/</link>
      <pubDate>Sun, 21 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://mateojuliani.github.io/posts/2025_12_14_6701_readings/</guid>
      <description>&lt;p&gt;This semester for Columbia&amp;rsquo;s STCS6701: Probabilistic Models and Machine Learning taught by David Blei, I read a variety of papers on probabilistic machine learning. Below I include some short write ups on my thoughts for some of the papers, organized by topics.&lt;/p&gt;&#xA;&lt;h1 id=&#34;bayesian-inspired-regularizers-for-neural-nets&#34;&gt;Bayesian Inspired Regularizers for Neural Nets&lt;/h1&gt;&#xA;&lt;h2 id=&#34;curvature-driven-smoothing-a-learning-algorithm-for-feedforward-networkshttpsieeexploreieeeorgdocument248466-bayesian-back-propagation-section-5httpswwwsemanticscholarorgpaperbayesian-back-propagation-buntine-weigendc83684f6207697c12850db423fd9747572cf1784&#34;&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/248466/&#34;&gt;Curvature-Driven Smoothing: A Learning Algorithm for Feedforward Networks&lt;/a&gt;, &lt;a href=&#34;https://www.semanticscholar.org/paper/Bayesian-Back-Propagation-Buntine-Weigend/c83684f6207697c12850db423fd9747572cf1784&#34;&gt;Bayesian Back-Propagation (Section 5)&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;These two papers explored different priors that could be used with Neural Nets. The first paper, focused on creating a functional smoothness prior based on the second derivative of a neural net with respect to the input. This prior is added to the traditional MSE loss function, thus the learning algorithm will penalize neural nets with large &amp;ldquo;curvatures&amp;rdquo; or changes in magnitudes, thus potentially learning a smoother function. The second paper focuses on smoothing priors and entropic priors (priors that nudge the model away from being very confident in a single estimate).&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Simple - and Somewhat Effective - Soccer Model</title>
      <link>https://mateojuliani.github.io/posts/international_soccer_forecasts_5_24_25/</link>
      <pubDate>Sat, 24 May 2025 00:00:00 +0000</pubDate>
      <guid>https://mateojuliani.github.io/posts/international_soccer_forecasts_5_24_25/</guid>
      <description>&lt;p&gt;Ahead of the next international soccer break in early June 2025, I wanted to publish a quick overview of the methodology I use to create international soccer forecasts on &lt;a href=&#34;https://mateojuliani.github.io/projects/xg_boosted/&#34;&gt;this site&lt;/a&gt;. If you have any thoughts on the below, I would love to hear from you at mateojuliani at gmail dot com!&lt;/p&gt;&#xA;&lt;h1 id=&#34;section-1-intro-and-motivation&#34;&gt;Section 1: Intro and Motivation&lt;/h1&gt;&#xA;&lt;p&gt;In November 2023, I came across these betting odds for the World Cup Qualifying match between Brazil and Argentina:&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Current State of Agentic Systems Research</title>
      <link>https://mateojuliani.github.io/posts/6113_1_26_25/</link>
      <pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://mateojuliani.github.io/posts/6113_1_26_25/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Papers Read / Referenced&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/nature24270&#34;&gt;Mastering the game of Go without human knowledge&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.02052&#34;&gt;ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.06770&#34;&gt;SWE-bench: Can Language Models Resolve Real-World GitHub Issues?&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.15793&#34;&gt;SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.03714&#34;&gt;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Over the last few weeks, I have started reading more papers on AI Agents and Agentic Systems. Below I am including my thoughts on all of the papers and synthesizing my thoughts on the current state of research in the field.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Notes on OSWorld and SWE-Agent</title>
      <link>https://mateojuliani.github.io/posts/6113_1_23_25/</link>
      <pubDate>Sat, 25 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://mateojuliani.github.io/posts/6113_1_23_25/</guid>
      <description>&lt;p&gt;&lt;!-- raw HTML omitted --&gt;Papers Read&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.15793&#34;&gt;SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;This week for my Agentic Systems Made Real class, we read two papers, &lt;a href=&#34;https://arxiv.org/abs/2405.15793&#34;&gt;SWE-agent&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;OSWorld&lt;/a&gt;. Overall, I thought both papers provided a good overview of the efficacy of agents with certain tasks and the challenges researchers face to create them. The field is moving quickly (OpenAI released &lt;a href=&#34;https://openai.com/index/computer-using-agent/&#34;&gt;Operator&lt;/a&gt; that achieved 38.1% success rate on OSWorld vs 12.2% in the original paper) so some of the observations might be slightly outdated now, but adding them below.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Read a Paper</title>
      <link>https://mateojuliani.github.io/posts/how_to_read_paper/</link>
      <pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://mateojuliani.github.io/posts/how_to_read_paper/</guid>
      <description>&lt;p&gt;This semester at Columbia, I am taking two research focused classes on Causal Inference and AI Agents. Reading academic papers is a main component of both classes. While in the past I have read research papers, I did not have a set system to efficiently read, understand, and annotate the papers. The purpose of this post is to detail my current method for my own reference, to source feedback from others (see about page), and (maybe!) help others read papers more efficiently.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
